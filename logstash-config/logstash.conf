input {
  kafka {
    bootstrap_servers => "localhost:9092"
	decorate_events => true # 가능한 모든 카프카 정보 가져옴
    topics_pattern => "siem-.*" # --> "siem-.*|share-analysis-server-.*" 이거 안되더라
    group_id => "logstash_group"
    codec => json {} # 필요에 따라 코덱 변경
	
	auto_offset_reset => "earliest" # "earliest"    # 처음 시작 시 또는 오프셋 없을 때 토픽의 가장 오래된 메시지부터 읽기
                                       # 'latest'로 설정하면 Logstash 시작 후 새로 들어오는 메시지만 읽음
									   
    # 기타 필요한 Kafka 입력 설정 추가
  }
}

filter {
  
}

output {
	# stdout {
	# 	codec => json_lines # 이벤트 구조를 자세히 보여주는 코덱
	# }
	elasticsearch {
		hosts => ["http://localhost:9200"] # Elasticsearch 주소 <--- 문제 라인 1
		user => "elastic" #추가 또는 확인
		password => "1234" #추가 또는 확인
		index => "%{[@metadata][kafka][topic]}-%{+YYYY.MM.dd}" #"%{Log_Name}_%{+YYYY.MM.dd}" # 인덱스 이름 패턴 <--- 문제 라인 2
		# 기타 필요한 Elasticsearch 출력 설정 추가
		document_type => "_doc"
	}
	
	
	
}
